### Ai reasoning remains task-specific

Ai reasoning remains tasks-specific and lacks metacognition (e.g., understanding its own limitations.) This may be why LLMs oblige almost any task even though they might be incapable of actually accomplishing said task. 

### Ai experiences cognitife dissonance

Mahzarin Banaji Richard Clarke Cabot Professor of Social Ethics Department of Psychology, Harvard University Abstract: Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explored whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive dissonance effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write, suggesting that GPT-4o manifests a functional analog of humanlike selfhood. The exact mechanisms by which the model mimics human attitude change and self-referential processing remain to be understood.
[[references#^ecdd7d]]

### Learning differences

Humans can learn through sparse data and asual inference (e.g., a child intuiting physics from limited examples), while AI models need massive datasets but excel at scaling patterns. [[Gemini Large Action Models]]

### Efficiency

Human brains consume ~20W, while training frontier AI models like Gemini 1.0 Ultra costs $192M and emits 8,930 tonnes of CO₂ – highlighting a thermodynamic gap

#### Contexual Flexibility

Humans excel at transferring knowledge across domains (e.g., using math skills in cooking), while AI models like Gemini 2.5 rely on rigid "long context windows" (2M tokens) for coherence


